{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc7T2LCpqlQ2"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOIRq-hLqmM_"
      },
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C2-white-bg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hls1cHzjqZfN"
      },
      "source": [
        "# Lab: Implement a BPE Tokenizer\n",
        "\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_4_implement_a_bpe_tokenizer.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Implement a subword tokenizer using the byte pair encoding (BPE) algorithm.\n",
        "\n",
        "25 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX6LqVz0IN_G"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will implement the **byte pair encoding (BPE) algorithm** that was presented in the previous activity. This will provide you with a  more sophisticated tokenizer than the space tokenizer that you have been using in previous labs. This in return will allow you to train a better model using the Africa Galore dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Cl--jGzR9z"
      },
      "source": [
        "### What you will learn\n",
        "\n",
        "By the end of this lab, you will understand:\n",
        "\n",
        "* The input and output of the byte pair tokenization (BPE) algorithm.\n",
        "* The iterative steps involved in learning a tokenizer.\n",
        "* How to implement and run the full BPE algorithm over multiple merge steps.\n",
        "\n",
        "\n",
        "### Tasks\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Write code to convert each word into a list of characters with `</w>` appended.\n",
        "* Complete the function `get_pair_frequencies` to count adjacent symbol pairs.\n",
        "* Fill in logic in `merge_pair_in_word` to replace the most frequent pair.\n",
        "* Apply merges iteratively to update the corpus and vocabulary.\n",
        "* Implement the loop in `learn_bpe` to perform a fixed number of merge steps and inspect results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Mc6UzdSbSJ"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJikYa4iScuY"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over the cell and click on the `run` button to its left. The run button is the circle with the triangle (‚ñ∂). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or ‚åò+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQunPUsRShVa"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKpR1cc0Se9B"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime ‚Üí Run before__  from the menu above (or use the keyboard combination Ctrl/‚åò + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JoWmtemA8eZ"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2B3vqDCwd2vj"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict # For counting token pairs.\n",
        "import pandas as pd # For loading the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs3Efa3zxmFX"
      },
      "source": [
        "## Byte pair encoding (BPE)\n",
        "\n",
        "Recall from the previous article that the BPE algorithm consists of the following steps:\n",
        "\n",
        "1. **Initialization**: The entire dataset (a text corpus) is split into individual characters and the vocabulary is initialized with the set of unique characters. Spaces are replaced with a special end-of-word symbol (e.g., `</w>`).\n",
        "2. **Counting**: For each adjacent pair of tokens, count how often each pair appears in your corpus.\n",
        "3. **Merging**: Choose the adjacent pair of tokens `(p, q)` that appears most frequently in the corpus and add a new merged token `pq` to the vocabulary.\n",
        "4. **Replacing**: Replace all adjacent pairs of tokens `(p, q)` with the new token `pq` in the corpus.\n",
        "5. **Repeating**: Repeat steps 2-4 until you reach the target vocabulary size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8DD-hCgxuAl"
      },
      "source": [
        "### Coding Activity 1: Initialization\n",
        "\n",
        "You will use the Africa Galore dataset to perform one step of the BPE algorithm before running the algorithm for multiple iterations on a smaller corpus.\n",
        "\n",
        "Run the following cell to load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fQDarr35PpBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7a79c4-13f3-4bb6-e91f-9499862d1c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contains 232 paragraphs.\n"
          ]
        }
      ],
      "source": [
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "dataset = africa_galore[\"description\"].values\n",
        "print(f\"Dataset contains {dataset.shape[0]:,} paragraphs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2CRoQYfFGI9"
      },
      "source": [
        "The initialization step consists of three parts:\n",
        "1. Converting every space-separated word into a list of characters.\n",
        "2. Appending the special end-of-word symbol `\"</w>\"` at the end of each word's list of characters.\n",
        "3. Adding the set of unique characters to your vocabulary.\n",
        "\n",
        "<br />\n",
        "\n",
        "-----\n",
        "> **üíª Your task:**\n",
        ">\n",
        "> Complete the `bpe_initialize` function below such that `corpus` contains a list with each word in the dataset represented as a list of characters and the end-of-word symbol.\n",
        ">\n",
        "> Hint: You can turn a string `s` into a list of characters with the `list` function: `list(s)`.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AcOp5WyIyUiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d8abd4-31a8-46be-9f89-fc42496f76ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 words: \n",
            "  ['T', 'h', 'e', '</w>']\n",
            "  ['L', 'a', 'g', 'o', 's', '</w>']\n",
            "  ['a', 'i', 'r', '</w>']\n",
            "  ['w', 'a', 's', '</w>']\n",
            "  ['t', 'h', 'i', 'c', 'k', '</w>']\n",
            "  ['w', 'i', 't', 'h', '</w>']\n",
            "  ['h', 'u', 'm', 'i', 'd', 'i', 't', 'y', ',', '</w>']\n",
            "  ['b', 'u', 't', '</w>']\n",
            "  ['t', 'h', 'e', '</w>']\n",
            "  ['e', 'n', 'e', 'r', 'g', 'y', '</w>']\n",
            "\n",
            "\n",
            "Vocabulary:\n",
            "  {'u', 'F', '3', '‚Äî', 'M', '0', 'K', 'c', 'X', 'v', ';', 'P', 'W', 'z', 'f', 'I', 'H', 'p', 'N', '\"', 'U', 't', 'd', 'g', ',', 's', 'j', '¬∞', '‚Äú', '(', ':', '</w>', 'Z', '5', '?', 'm', 'h', '9', '.', 'y', 'A', 'D', 'E', 'r', 'e', '/', 'L', 'n', ')', 'x', '√©', 'k', '4', 'o', 'V', \"'\", '6', 'B', '-', '‚Äù', 'q', 'w', 'C', 'l', 'G', 'b', 'a', '7', 'T', '8', 'R', '1', 'J', 'O', '2', 'i', 'S', 'Y'}\n"
          ]
        }
      ],
      "source": [
        "EOW_SYMBOL = \"</w>\"\n",
        "\n",
        "def bpe_initialize(dataset: list[str]) -> tuple[list[list[str]], set[str]]:\n",
        "    \"\"\"Implements the initialization step of the byte pair encoding algorithm.\n",
        "\n",
        "    Args:\n",
        "      dataset: The corpus consisting of a list of raw paragraphs.\n",
        "\n",
        "    Returns:\n",
        "      corpus: A list containing every word in the corpus represented as a list\n",
        "        of characters and the special end-of-word-symbol \"</w>\".\n",
        "      vocabulary: The set of unique tokens in the dataset that serves as the\n",
        "        vocabulary before the first merge.\n",
        "    \"\"\"\n",
        "\n",
        "    corpus = []\n",
        "    vocabulary = {EOW_SYMBOL}\n",
        "    for paragraph in dataset:\n",
        "        for word in paragraph.split(\" \"):\n",
        "            # Convert word into chars and add end-of-word symbol ('</w>').\n",
        "            chars =  list(word) + [EOW_SYMBOL]\n",
        "            corpus.append(chars)\n",
        "\n",
        "            # Append characters to the vocabulary. Since vocabulary is a set,\n",
        "            # it will ignore characters that have already been added.\n",
        "            vocabulary.update(chars)\n",
        "\n",
        "    return corpus, vocabulary\n",
        "\n",
        "corpus, vocabulary = bpe_initialize(dataset)\n",
        "\n",
        "# Print the first 10 \"words\" in the corpus.\n",
        "print(\"First 10 words: \")\n",
        "for word in corpus[:10]:\n",
        "    print(f\"  {word}\")\n",
        "\n",
        "print(\"\\n\\nVocabulary:\")\n",
        "print(f\"  {vocabulary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVcZIOgxqUDf"
      },
      "source": [
        "\n",
        "### Coding Activity 2: Counting adjacent token pairs\n",
        "\n",
        "The second step of the BPE algorithm is to count adjacent token pairs. For every word in the corpus, you have to extract every pair of tokens that appear next to each other and increase their count. For example, when encountering the word represented as `[\"r\", \"i\", \"c\", \"e\", \"</w\">]`, the counts of the following four pairs should be incremented by 1: `(r,i)`, `(i,c)`, `(c,e)`, `(e, </w>)`.\n",
        "\n",
        "<br />\n",
        "\n",
        "-------\n",
        "> **üíª Your task:**\n",
        ">\n",
        "> Complete the `get_pair_frequencies` function below.\n",
        ">\n",
        "> This function should return a [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) object whose keys are pairs of tokens and the value is the frequency of the pair in the corpus.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dAMDJZ7Eu6Tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5744adab-384e-481d-b9d8-d2d7f2554095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 10 most common pair frequencies are:\n",
            " (('e', '</w>'), 2639)\n",
            " (('d', '</w>'), 2146)\n",
            " (('s', '</w>'), 2078)\n",
            " (('a', 'n'), 1883)\n",
            " (('t', 'h'), 1869)\n",
            " (('i', 'n'), 1822)\n",
            " (('h', 'e'), 1735)\n",
            " ((',', '</w>'), 1710)\n",
            " (('e', 'r'), 1359)\n",
            " (('n', 'd'), 1305)\n"
          ]
        }
      ],
      "source": [
        "def get_pair_frequencies(\n",
        "    corpus: list[list[str]]\n",
        ") -> Counter[tuple[str, str], int]:\n",
        "    \"\"\"\n",
        "    Calculates the frequency of adjacent character pairs in a corpus.\n",
        "\n",
        "    Args:\n",
        "      corpus: A list of tokenized words, where each word is represented as a\n",
        "        list of subword tokens. Before the first merge these are all individual\n",
        "        characters.\n",
        "\n",
        "    Returns:\n",
        "      A Counter object mapping each adjacent character pair (a tuple) to its\n",
        "        frequency in the corpus.\n",
        "    \"\"\"\n",
        "    pairs = Counter()\n",
        "    for word in corpus:\n",
        "        # Loop through the position of every token but the last one.\n",
        "        for i in range(len(word) - 1):\n",
        "            # Create a tuple representing the adjacent pair consisting of the\n",
        "            # i-th and (i+1)-th token in `word`.\n",
        "            pair =  (word[i], word[i+1])\n",
        "            pairs[pair] += 1\n",
        "    return pairs\n",
        "\n",
        "\n",
        "pair_freqs = get_pair_frequencies(corpus)\n",
        "print(\"The 10 most common pair frequencies are:\")\n",
        "for freq in pair_freqs.most_common(10):\n",
        "    print(f\" {freq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9umz2NWt2-yE"
      },
      "source": [
        "The output shows the most frequent adjacent character pairs and their counts in the corpus. The pairs with `</w>` indicate characters that frequently appear at the end of words. For example, `(\"e\", \"</w>\")` is the most frequent pair, meaning that many words in the corpus end with the letter \"e\". Other common pairs like `(\"i\", \"n\")`, `(\"a\", \"n\")`, and `(\"t\", \"h\")` represent frequent character combinations within words. This information is used in the next step of the BPE algorithm to merge the most frequent pairs into new subword tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QAkeFPw3LZM"
      },
      "source": [
        "### Coding Activity 3: Merge the most frequent pair\n",
        "\n",
        "The third step of the BPE algorithm is to identify the most frequent pair of adjacent tokens and to merge them into a new subword token.\n",
        "\n",
        "Run the following cell to identify the most frequent token pair and add it to the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gJ-D2kVO3Wc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67510da6-5176-4233-b08c-036270aba1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most frequent pair is ('e', '</w>') with a frequency of 2,639.\n"
          ]
        }
      ],
      "source": [
        "most_freq_pair, freq = pair_freqs.most_common(1)[0]\n",
        "print(\n",
        "    f\"The most frequent pair is {most_freq_pair} with a frequency of {freq:,}.\"\n",
        ")\n",
        "\n",
        "new_token = most_freq_pair[0] + most_freq_pair[1]\n",
        "vocabulary.add(new_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47F59nBDkQCC"
      },
      "source": [
        "------\n",
        "> **üíª Your task:**\n",
        ">\n",
        "> Complete the `merge_pair_in_word` function below.\n",
        ">\n",
        "> This function should replace all instances of `pair_to_merge`\n",
        "> in `word` with a new subword token that consists of the two\n",
        "> tokens in `pair_to_merge`.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OlLRmDgj_7y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c448d4-49ac-49b4-b909-597aea46ab24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before merging: T h e </w>\n",
            "After merging:  T h e</w>\n"
          ]
        }
      ],
      "source": [
        "def merge_pair_in_word(\n",
        "    word: list[str], pair_to_merge: tuple[str, str]\n",
        ") -> list[str]:\n",
        "    \"\"\"Merges adjacent occurrences of a specified pair of characters in a word.\n",
        "\n",
        "    Given a word represented as a list of subword tokens and a pair of tokens to\n",
        "    merge (represented as a tuple), this function returns a new list where every\n",
        "    instance of the specified pair appearing adjacently in the original word is\n",
        "    replaced by a single string representing the merged pair.\n",
        "\n",
        "    Args:\n",
        "      tokens: A list of subword tokens representing one space separated word.\n",
        "      pair_to_merge: A pair of two subword tokens that should be merged into one\n",
        "        subword token.\n",
        "\n",
        "    Returns:\n",
        "      New list of subword tokens representing the word after applying the merge.\n",
        "    \"\"\"\n",
        "    merged_symbol = pair_to_merge[0] + pair_to_merge[1]\n",
        "    i = 0\n",
        "    new_word = []\n",
        "    while i < len(word):\n",
        "        # If this position and the next match the pair, merge them.\n",
        "        if i < len(word) - 1 and (word[i],word[i+1]) == pair_to_merge:\n",
        "            new_word.append(merged_symbol)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_word.append(word[i])\n",
        "            i += 1\n",
        "    return new_word\n",
        "\n",
        "print(f\"Before merging: {' '.join(corpus[0])}\")\n",
        "new_word = merge_pair_in_word(corpus[0], most_freq_pair)\n",
        "print(f\"After merging:  {' '.join(new_word)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjwDB-KTAkBN"
      },
      "source": [
        "### Replace tokens in corpus\n",
        "\n",
        "Once the new token has been added to the vocabulary, you can use the `merge_pair_in_word` function that you implemented above to replace every occurrence of the most frequent pair with the new merged subword token. This is the fourth step of the BPE algorithm.\n",
        "\n",
        "Run the following cell to perform the merges in the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4TXiNODS31m0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d3c58e-a48e-4e15-ce68-97f27f31f9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 \"words\" in the corpus, with \"e</w>\" being a single token now:\n",
            "  ['T', 'h', 'e</w>']\n",
            "  ['L', 'a', 'g', 'o', 's', '</w>']\n",
            "  ['a', 'i', 'r', '</w>']\n",
            "  ['w', 'a', 's', '</w>']\n",
            "  ['t', 'h', 'i', 'c', 'k', '</w>']\n",
            "  ['w', 'i', 't', 'h', '</w>']\n",
            "  ['h', 'u', 'm', 'i', 'd', 'i', 't', 'y', ',', '</w>']\n",
            "  ['b', 'u', 't', '</w>']\n",
            "  ['t', 'h', 'e</w>']\n",
            "  ['e', 'n', 'e', 'r', 'g', 'y', '</w>']\n",
            "\n",
            "\n",
            "Vocabulary:\n",
            "  {'u', 'F', '3', '‚Äî', 'M', '0', 'K', 'c', 'X', 'v', ';', 'P', 'W', 'z', 'f', 'I', 'H', 'p', 'N', '\"', 'U', 't', 'd', 'g', ',', 's', 'j', '¬∞', '‚Äú', '(', ':', '</w>', 'Z', '5', '?', 'm', 'h', '9', '.', 'y', 'A', 'D', 'E', 'r', 'e', '/', 'L', 'n', ')', 'x', '√©', 'k', '4', 'o', 'V', \"'\", '6', 'B', '-', '‚Äù', 'q', 'w', 'C', 'l', 'G', 'b', 'a', '7', 'T', '8', 'R', '1', 'J', 'O', '2', 'i', 'S', 'Y', 'e</w>'}\n"
          ]
        }
      ],
      "source": [
        "new_corpus = []\n",
        "for word in corpus:\n",
        "    new_word = merge_pair_in_word(word, most_freq_pair)\n",
        "    new_corpus.append(new_word)\n",
        "\n",
        "# Each iteration of BPE results in a new corpus with merged pairs.\n",
        "print(\n",
        "    \"The first 10 \\\"words\\\" in the corpus, with \\\"e</w>\\\" being a single token\"\n",
        "    \" now:\"\n",
        ")\n",
        "for word in new_corpus[:10]:\n",
        "    print(f\"  {word}\")\n",
        "\n",
        "print(\"\\n\\nVocabulary:\")\n",
        "print(f\"  {vocabulary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImXMw2oGohE8"
      },
      "source": [
        "Note how both instances of the word \"the\" are now split into three subword tokens because the final \"e\" has been merged with the end-of-word token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCpyeFDOCVrI"
      },
      "source": [
        "### Repeat\n",
        "\n",
        "The steps 2, 3, and 4 can now be repeated for either a fixed number of steps or until you reach a target vocabulary size.\n",
        "\n",
        "On each iteration, exactly one pair of tokens is merged resulting in one more subword token in the vocabulary.\n",
        "\n",
        "The function below implements the complete BPE algorithm that runs for a fixed number of `num_merges`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iMQ8NnosrXGV"
      },
      "outputs": [],
      "source": [
        "def learn_bpe(\n",
        "    dataset: list[str], num_merges: int\n",
        ") -> tuple[list[tuple[str, str]], list[list[str]]]:\n",
        "    \"\"\"\n",
        "    Learns byte pair encoding (BPE) merge operations from a dataset.\n",
        "\n",
        "    This function implements the basic BPE algorithm, iteratively merging the\n",
        "    most frequent adjacent character pairs in a corpus to create subword units.\n",
        "    It continues merging until the specified number of merges is reached or all\n",
        "    words are represented by a single token.\n",
        "\n",
        "    Args:\n",
        "      dataset: A list of paragraphs, where each sentence is a string.\n",
        "      num_merges: The desired number of BPE merge operations to perform.\n",
        "\n",
        "    Returns:\n",
        "      merges: A list of tuples, where each tuple represents a merged pair of\n",
        "        tokens. (e.g., [('a', 'b'), ('ab', 'c')]).  The order of tuples reflects\n",
        "        the order in which merges were performed.\n",
        "      vocabulary: The final vocabulary of all subword tokens.\n",
        "      corpus: The final corpus after BPE is applied.  Each sentence is\n",
        "        represented as a list of words, and each word is represented as a list\n",
        "        of subword tokens.\n",
        "    \"\"\"\n",
        "    # Step 1: Initialize corpus as list of lists of characters + </w>.\n",
        "    corpus, vocabulary = bpe_initialize(dataset)\n",
        "\n",
        "    merges = []\n",
        "    for _ in range(num_merges):\n",
        "        # Step 2: Count all adjacent-pair frequencies.\n",
        "        pair_freqs = get_pair_frequencies(corpus)\n",
        "        if not pair_freqs:\n",
        "            break\n",
        "\n",
        "        # Step 3: Pick the most frequent pair.\n",
        "        most_freq_pair, freq = pair_freqs.most_common(1)[0]\n",
        "        if freq < 1:\n",
        "            break  # No more pairs to merge.\n",
        "        merges.append(most_freq_pair)\n",
        "        new_token = most_freq_pair[0] + most_freq_pair[1]\n",
        "        vocabulary.add(new_token)\n",
        "\n",
        "        # Step 4: Merge that pair in every word of the corpus.\n",
        "        new_corpus = []\n",
        "        for word in corpus:\n",
        "            new_word = merge_pair_in_word(word, most_freq_pair)\n",
        "            new_corpus.append(new_word)\n",
        "        corpus = new_corpus\n",
        "\n",
        "    # Return the list of merges, the vocabulary, and the final tokenized corpus.\n",
        "    return merges, vocabulary, corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4zR1S3tREA"
      },
      "source": [
        "### Run BPE on a small corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TXtwq3V1r6J"
      },
      "source": [
        "Run the following cell to apply your BPE algorithm to a small collection of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-W0YOcc4rq0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df630836-d981-42dd-fdad-f755b3ab48e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned merges (in order):\n",
            " 1. Merge 'e' + 's'  ‚Üí  'es'\n",
            " 2. Merge 'd' + 'es'  ‚Üí  'des'\n",
            " 3. Merge 'des' + 'e'  ‚Üí  'dese'\n",
            " 4. Merge 'dese' + 'r'  ‚Üí  'deser'\n",
            " 5. Merge 'deser' + 't'  ‚Üí  'desert'\n",
            " 6. Merge 'desert' + '</w>'  ‚Üí  'desert</w>'\n",
            " 7. Merge 'e' + 'd'  ‚Üí  'ed'\n",
            " 8. Merge 'ed' + '</w>'  ‚Üí  'ed</w>'\n",
            " 9. Merge 'i' + 'o'  ‚Üí  'io'\n",
            " 10. Merge 'io' + 'n'  ‚Üí  'ion'\n",
            "\n",
            "Final tokenized corpus:\n",
            "  desert   ‚Üí desert</w>\n",
            "  deserted ‚Üí desert ed</w>\n",
            "  deserts  ‚Üí desert s </w>\n",
            "  desert   ‚Üí desert</w>\n",
            "  tested   ‚Üí t es t ed</w>\n",
            "  test     ‚Üí t es t </w>\n",
            "  deserted ‚Üí desert ed</w>\n",
            "  desert   ‚Üí desert</w>\n",
            "  desertion ‚Üí desert ion </w>\n",
            "  desertion ‚Üí desert ion </w>\n",
            "  function ‚Üí f u n c t ion </w>\n"
          ]
        }
      ],
      "source": [
        "sample_text = [\n",
        "        \"desert\",\n",
        "        \"deserted\",\n",
        "        \"deserts\",\n",
        "        \"desert\",\n",
        "        \"tested\",\n",
        "        \"test\",\n",
        "        \"deserted\",\n",
        "        \"desert\",\n",
        "        \"desertion\",\n",
        "        \"desertion\",\n",
        "        \"function\",\n",
        "]\n",
        "# Learn BPE tokenizer with 10 merge operations.\n",
        "merges, vocabulary, tokenized_corpus = learn_bpe(sample_text, num_merges=10)\n",
        "\n",
        "print(\"Learned merges (in order):\")\n",
        "for i, pair in enumerate(merges, start=1):\n",
        "    print(f\" {i}. Merge {pair[0]!r} + {pair[1]!r}  ‚Üí  {pair[0]+pair[1]!r}\")\n",
        "\n",
        "print(\"\\nFinal tokenized corpus:\")\n",
        "for orig, tokenized in zip(sample_text, tokenized_corpus):\n",
        "    print(f\"  {orig:8s} ‚Üí {' '.join(tokenized)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKI3lzRMiHLj"
      },
      "source": [
        "You have now successfully trained your own BPE tokenizer on a small corpus. In later labs, you will incorporate this algorithm into a class which can tokenize any given text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "729bb1a7"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you implemented the **byte pair encoding (BPE) algorithm**, a cornerstone of modern text tokenization. Through this implementation, you have observed how BPE iteratively **merges** frequent character pairs to create **subword tokens**. This process is crucial for handling the vast and dynamic nature of human language. It is especially important in the context of **large vocabulary sizes** and the need to represent **out-of-vocabulary tokens**. Understanding BPE and other subword tokenization techniques is critical for anyone working with language models. It directly impacts model performance, efficiency, and the ability to handle diverse text data.\n",
        "\n",
        "In the next module of this course, you will explore how language models encode meaning in a high-dimensional vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuKuDgtqtZjl"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZAtonCtboL"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPC9ZMm8thCb"
      },
      "outputs": [],
      "source": [
        "# Complete implementation of the bpe_initialize function.\n",
        "def bpe_initialize(dataset: list[str]) -> tuple[list[list[str]], set[str]]:\n",
        "    \"\"\"Implements the initialization step of the byte pair encoding algorithm.\n",
        "\n",
        "    Args:\n",
        "      dataset: The corpus consisting of a list of raw paragraphs.\n",
        "\n",
        "    Returns:\n",
        "      corpus: A list containing every word in the corpus represented as a list\n",
        "        of characters and the special end-of-word-symbol \"</w>\".\n",
        "      vocabulary: The set of unique tokens in the dataset that serves as the\n",
        "        vocabulary before the first merge.\n",
        "    \"\"\"\n",
        "\n",
        "    corpus = []\n",
        "    vocabulary = {EOW_SYMBOL}\n",
        "    for paragraph in dataset:\n",
        "        for word in paragraph.split(\" \"):\n",
        "            # Convert word into chars and add end-of-word symbol ('</w>').\n",
        "            chars = list(word) + [EOW_SYMBOL]\n",
        "            corpus.append(chars)\n",
        "            # Add all characters to the vocabulary. Since vocabulary is a set,\n",
        "            # it will ignore characters that have already been added.\n",
        "            vocabulary.update(chars)\n",
        "\n",
        "    return corpus, vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoN4SRj5hI_u"
      },
      "source": [
        "### Coding Activity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaeIDU8ShKsC"
      },
      "outputs": [],
      "source": [
        "# Complete implementation of the get_pair_frequencies function.\n",
        "def get_pair_frequencies(\n",
        "    corpus: list[list[str]]\n",
        ") -> Counter[tuple[str, str], int]:\n",
        "    \"\"\"Calculates the frequency of adjacent character pairs in a corpus.\n",
        "\n",
        "    Args:\n",
        "      corpus: A list of tokenized words, where each word is represented as a\n",
        "        list of subword tokens. Before the first merge these are all individual\n",
        "        characters.\n",
        "\n",
        "    Returns:\n",
        "      A Counter object mapping each adjacent character pair (a tuple) to its\n",
        "        frequency in the corpus.\n",
        "    \"\"\"\n",
        "    pairs = Counter()\n",
        "    for word in corpus:\n",
        "        # Loop through the position of every token but the last one.\n",
        "        for i in range(len(word) - 1):\n",
        "            # Create a tuple representing the adjacent pair consisting of the\n",
        "            # i-th and (i+1)-th token in `word`.\n",
        "            pair = (word[i], word[i+1])\n",
        "            pairs[pair] += 1\n",
        "    return pairs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjGEOIfOtfe9"
      },
      "source": [
        "### Coding Activity 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiudMLMhtiW0"
      },
      "outputs": [],
      "source": [
        "# Complete implementation of the merge_pair_in_word function.\n",
        "def merge_pair_in_word(word: list[str], pair_to_merge: tuple[str, str]) -> list[str]:\n",
        "    \"\"\"Merges adjacent occurrences of a specified pair of characters in a word.\n",
        "\n",
        "    Given a word represented as a list of subword tokens and a pair of tokens to\n",
        "    merge (represented as a tuple), this function returns a new list where every\n",
        "    instance of the specified pair appearing adjacently in the original word is\n",
        "    replaced by a single string representing the merged pair.\n",
        "\n",
        "    Args:\n",
        "      tokens: A list of subword tokens representing one space separated word.\n",
        "      pair_to_merge: A pair of two subword tokens that should be merged into one\n",
        "        subword token.\n",
        "\n",
        "    Returns:\n",
        "      New list of subword tokens representing the word after applying the merge.\n",
        "\n",
        "    \"\"\"\n",
        "    merged_symbol = pair_to_merge[0] + pair_to_merge[1]\n",
        "    i = 0\n",
        "    new_word = []\n",
        "    while i < len(word):\n",
        "        # If this position and the next match the pair, merge them.\n",
        "        if i < len(word) - 1 and (word[i], word[i + 1]) == pair_to_merge:\n",
        "            new_word.append(merged_symbol)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_word.append(word[i])\n",
        "            i += 1\n",
        "    return new_word"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "C8Mc6UzdSbSJ",
        "LuKuDgtqtZjl",
        "7sZAtonCtboL",
        "hoN4SRj5hI_u",
        "VjGEOIfOtfe9"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}